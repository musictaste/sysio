[TOC]


### 虚拟文件系统VFS

就是==一棵树==，树上的不同节点可以映射到不同的物理地址，每一不同的物理地址可以是不同的具体的文件系统

知识点1：**两个程序打开同一个文件的话，不是两个程序都把这个文件都加载一次**，其实是**访问的虚拟文件系统VFS**，如果虚拟文件系统发现**已经加载过这个文件了**，你读的东西内存的pagecache已经命中了，那么**虚拟文件系统就会返回你要的pagecache数据**

知识点2：程序和硬件之间隔了一个内核，**内核通过pagecache来维护你曾经想读的数据在内存里，如果对数据进行了修改，会标记为dirty；标记为dirty以后，会有一个flush的过程，书写到磁盘当中去；**

### 文件描述符FD

文件描述符：就是**描述了你打开文件的一些信息**，例如文件类型，指针、偏移量、Inode、文件

内核为每一个进程各自维护了一套数据，这套数据包含了进程的FD（文件描述符），FD里维护了一些关于FD的信息，有偏移量、指针位置、类型、文件名、所处设备的位置、Inode

lsof -p 进程号

lsof -op 进程号

 seek ,文件描述符FD中的seek指针偏移量；大数据中HDFS也有seek，支撑起了分布式并行计算；
 
 因为每一个程序打开相同的文件，它们各自seek到自己的数据段里，去并行拿到自己的n份数据，并进行计算

### ==冯诺依曼提出的计算机组成：计算器 + 控制器（CPU）+ 主存储器(主存)+ 输入输出设备(I/O)==

### docker

文件类型：块设备

docker中是先有img镜像文件，img中要先放你的程序

docker是一个容器，是一个虚幻的东西，是一个云空间，里面跑的进程，没有操作系统内核什么的，所有的docker复用的是你物理机的内核

先有镜像(img)，才有容器的概念

镜像需要先挂载，然后读取进程里面的执行文件，然后再准备命名空间，把文件跑到命名空间中，程序才能跑起来，这就是源于文件系统的支撑

### 文件类型

    -：普通文件（可执行文件、图片、文本） REG
    d:目录
    l:连接(软连接、硬连接)
        ln /root/msb.txt  /root/xxoo.txt
        硬连接：在内存中的描述路径(path)是不一样的，但是是指向同一个物理文件；相当于两个变量指向了一个对象，通过任意一个变量对对象进行修改，另一个变量都能反映出来
        
        ln -s /root/xxoo.txt /root/msb.txt  ll时可以到msb.txt .> /root/xxoo.txt 多了一个指向
        软连接：在内存中的描述路径(path)是不一样的，但是是指向同一个物理文件
        
        stat msb.txt  可以看到Inode
        stat xxoo.txt  可以看到Inode是一样的
        rm -f msb.txt  xxoo.txt还在，但是xxoo前面的引用数量变为1（1就是硬连接被引用的数量）
    
    b:块设备(可以自由漂移，不受约束；硬盘)
        #生成一个mydisk.img的空文件，把它挂在到/dev/loop0, 进行了格式化；再mount到/mnt/ooxx，拷贝了一些bin和bash
        
        dd if=/dev/zero of=mydisk.img bs=1048576 count=100 输出一个100M的文件，被0填充
        losetup /dev/loop0 mydisk.img    将mydisk挂载到loop0(是一个设备，不是物理的，是一个虚拟的映射关系
        mke2fs /dev/loop0    格式化为ext2格式
        mount -t ext2 /dev/loop0  /mnt/ooxx  将loop0挂载到ooxx
        cp /lib64/{libtinfo.so.t,libdl.so.2,libc.so.6,ld-linux-x86-64.so.2} ./lib64/  花括号不允许出现任何的空白符
        chroot ./    启动程序
    
    c:字符设备(不能自由漂移；键盘、网卡)  CHR
    s:socket
        exec 8<> /dev/tcp/www.baidu.com/80   80端口，看似是一个目录，最终就是文件（一切皆文件），生成一个socket

    p:pipeline管道
        { a=9 ; echo "dfadf"; } | cat 
        管道的机制是：我写了一个文本字符串，回车；大家都知道bash是脚本解释执行，会解释这个文本段；解释的时候，发现管道，会在左边启动一个子进程，右边再启动一个子进程；并让两个进程的输入输出通过管道对接起来，所以“dfadf”能打印出来

        $$的优先级高过了管道，$BASHPID低于管道的级别；
        echo $$ | cat         4398是父进程号
        echo $BASHPID | cat    4496是新的子进程号
        
        
        { echo $BASHPID; read x; } | { cat; echo $BASHPID; read y;}        
        //输出：4512；用管道衔接了两个代码块; read x 和read y 会将进程阻塞
        4512是管道左边的子进程，父进程是4398
        ps -fe | grep 4398     发现进程4398有两个子进程：4512、4513
        cd /proc/4512/fd 看到文件操作符1指向了pipe[39968]
        cd /proc/4513/fd 看到文件操作符0指向了pipe[39968]
        lsof -op 4512      看到文件操作符1，是一个FIFO(先进先出)的pipe , node=39968
        lsof -op 4513      看到文件操作符0，也是一个FIFO的pipe
        
        
### 命令


```shell
df -h 查看分区挂载情况以及使用情况


mount /dev/sda1 /boot 将1分区的内容挂载到3分区的boot目录下
unmount /root 将root目录的内容卸载

dd if=/dev/zero of=mydisk.img bs=1048576 count=100 输出一个100M的文件，被0填充
losetup /dev/loop0 mydisk.img    将mydisk挂载到loop0(是一个设备，不是物理的，是一个虚拟的映射关系
mke2fs /dev/loop0    格式化为ext2格式
mount -t ext2 /dev/loop0  /mnt/ooxx  将loop0挂载到ooxx
cp /lib64/{libtinfo.so.t,libdl.so.2,libc.so.6,ld-linux-x86-64.so.2} ./lib64/  花括号不允许出现任何的空白符
chroot ./    启动程序

cd /proc/$$/fd  当前程序的所有文件描述符,可以看到0,1,2,255,8(指向了/root/ooxx.txt文件)

lsof -p $$  显示进程打开了哪些文件
lsof -op $$    -op(o是偏移量) 可以看到FD中有8r（r:读取）,可以看到Node-10227564(Inode)；OFFSET(指针)

echo "hello mashibing" > /abc.txt   打印字符串，利用重定向写到了本目录下的abc文件中
exec 8< ooxx.txt   利用FD 8读取ooxx文件
read a 0<& 8     读取的内容放到变量a中，0<& 8(标准输入流,从8读取)；
    read有一个特征：只读到换行符就不读了，所以它不会造成把整个文件都读完
exec 8<> /dev/tcp/www.baidu.com/80   80端口，看似是一个目录，最终就是文件（一切皆文件），生成一个socket
    
dd命令:备份、恢复、对拷、创建镜像
    dd if=/dev/sda3 of=~/abcd.img     将3分区（物理分区）的整个分区做了一个压缩文件的备份
        如果前面是物理硬盘，后面是一个文件，这个是备份
        如果前面是一个文件，后面是物理硬盘，这是恢复
        如果两个都是硬盘，是对拷
        如果前面是zero，后面是文件，是在创建一个空的磁盘镜像
        
    dd if=/dev/zero of=~/abcd.img    用0填充了一个空文件
    dd if=/dev/sda3 of=/dev/sdb1  两个分区对拷
    

输入输出操作符，左边为文件描述符，并且文件描述符和操作符之间不能有空格
ls ./ 1> ~/ls.out  bash程序的1(原先输入到屏幕)，修改为输出到ls.out
cat 0< ooxx.txt 1> cat.out    将输出修改为:写到cat.out文件
read a 0< cat.out   将变量a的输入修改为从文件读取内容，因为read对换行符极其敏感，所以只读入了aaa_

ls ./ /oosdfs     打印当前目录以及ooxx目录（ooxx目录并不存在）
        这是走了文件操作符1（标准输出）、2（报错输出）
ls ./ /oosdfs 1> ls01.out   将./的内容写到ls01文件中
ls ./ /oosdfs 1>ls01.out 2>ls02.out  这样屏幕就没有输入内容了
ls ./ /oosdfs 1>ls03.out 2>ls03.out  追加到一个文件
ls ./ /oosdfs 2>& 1 1>ls04.out   如果操作符要写文件操作符，一定要加&，该命令会报错..
ls ./ /oosdfs  1>ls04.out  2>& 1  1指向文件，2指向1，也指向了文件

head -2 test.txt      头两行
tail -2 test.txt      尾两行
head -8 test.txt | tail -1     显示第8行（先读头8行，交给管道，再读最后一行）

x=100  export x     让x具备导出能力  子进程读取到父进程的变量
这也就是为什么linux安装系统要修改etc/profile(vi /ect/profile)时，要定义环境变量时要加关键词export

$$的优先级高过了管道，$BASHPID低于管道的级别；
echo $$ | cat         4398是父进程号
echo $BASHPID | cat    4496是新的子进程号   

lsof -op $$ ：查看当前程序的所有文件描述符的细节

/proc/$$/fd : 当前程序的所有文件描述符

/proc:内核映射目录，映射内核的一些变量、属性
$$:当前bash的pid($BASHPID也可以得到pid),是一个特殊的环境变量
任何程序都有0(标准输入)、1（标准输出）、2（报错输出）


pcstat /bin/bash        pcstat:pageCache + stat;询问操作系统内核，bash文件有没有加载(填充)page页
pcstat out.txt
    利用pcstat查看一个文件从0到1的过程，它的缓冲率以及超过内存以后是否会淘汰一些page


sysctl -a | grep dirty       sysctl -a:显示centos的控制项
sysctl -p    将配置应用


netstat -natp 看内核socket建立的过程

tcpdump 抓取网络通讯数据包
tcpdump -nn -i eth0 port 9090 //服务端会开启LISTEN状态，监听端口号9090

starace -ff -o out  /usr/java/j2sdk1.4.2_18/bin/java TestSocket
    -ff  抓取所有跑的线程
    -o   给出输出文件的前缀，追踪每一个线程对内核的系统调用，并每一个线程都独立输出

yum install man man-pages
man man
man tcp
man ip
man bash
man 2 socket


route -n  服务端查看路由表


```


### DMA

**原先呢，硬盘数据 -> cpu的寄存器 -> kernel的pageCache**

**有了协处理器DMA（direct memory access直接内存访问）,数据可以不走cpu，实现硬盘数据直接到Kernal的pageCache**

![102](4B206DD2FBBE4B74874B96D6905FA106)

### system.call怎么实现的？   ？？？

### 80中断

Ox80 80中断

Ox80是一个值，存放在cpu的寄存器中，跟寄存器中的中断向量表匹配

中断向量表有255个（因为一个字节最大可以表示255）**，Ox80=128是call back方法

int Ox80一旦被cpu读到以后，会走call back，保护现场，恢复线程，从用户态切换到内核态


### 线程切换，挂起，中断过程

多个应用程序，如果一个应用程序running状态，调用了 系统调用，要从磁盘中读数据，

**这时cpu会调用int Ox80 系统中断，应用程序保护现场（保护现场：cpu 一二三级缓存的数据都保存到应用程序的缓冲栈）cpu的态也从用户态切换到内核态，开始执行kernal相应的系统调用，**

**你要读的数据，kernal要先分析pagecache中有没有这部分数据，pagecache没有触发缺页，走DMA协处理器，将数据从磁盘读取到pagecache中；这时呢，应用程序就会进入一种态：==挂起状态==，这时该应用程序就不在内核的进程调度中了(内核的进程调度是调用活跃的进程）**

DMA协处理器拷贝完数据，会有一个中断通知到cpu，告诉cpu曾经有一个应用程序在挂起的时候关注过这个数据；中断过来以后，会把应用程序的状态变为：==可运行状态==，未来在某个时间片进程再调度回来，这时应用程序再执行的时候，恢复现场，数据已经有了，就能够继续执行了


![103](C1651D9D3CF84D6683BE135628AD55B5)

# ======================================

### 面试题：epoll 怎么知道数据到达了，可以通知用户读取了？ ->中断

### PageCache

==为什么设计pagecache？是为了 减少硬件IO的频繁调用；为了提速；优先使用内存；==

**优点：优化IO性能；缺点：丢失数据**

pagecache在内核中，不是cpu中

**应用程序，内核、物理内存通过pagecache进行数据流转**

**应用程序和内核：通过文件描述符的偏移量seek**

**物理内存和内核：通过pagecache**

**缺页，产生缺页异常，产生软中断，用户态切换到内核态，cpu跑到内核，建立页映射关系后，再切换回用户态，程序继续执行**


---


pagecache是硬件设备(物理内存，真正存数据)和进程(应用程序)的一个抽象层

数据真正在磁盘上，内核中也有这个数据，内核中的数据提供给进程去使用



CPU的mmu单元：线性地址和物理地址的映射管理

缺页：cpu执行的时候会报缺页异常，类似一个软中断，cpu跑到内核，建立页映射关系，再漂移回来，cpu继续执行指令

缺页异常，会产生软中断，cpu从用户态切换到内核态，内核先去物理内存分配一个页，然后将线性地址指向这个物理地址，table中就记录了线性地址和物理地址的映射；然后CPU再从内核态切换到用户态，程序继续执行


```shell
pcstat /bin/bash        pcstat:pageCache + stat;询问操作系统内核，bash文件有没有加载(填充)page页、
```



### 应用程序，内核、物理内存通过pagecache进行数据流转


### pagecache调优

> 关联的知识点：
> 
> **理解redis 做持久化，尤其是AUF日志文件以及mysql调优的时候Binlog中有三个级别可以调：每秒写一次、随内核、每操作都写一次**
> 
> **就是因为内核不会立马写到磁盘**
> 

    sysctl -a | grep dirty       sysctl -a:显示centos的控制项
    vi /etc/sysctl.conf     下面中罗列了要修改的值
    sysctl -p    将配置应用
    
    
    //可用内存使用90%后，脏页从内存向磁盘写
    vm.dirty_background_ratio = 90  
    
    //程序使用内核可用内存占比90%，程序不再往内核内存中写数据;
    //dirty_background_ratio设置的值应小于dirty_ratio设置的值
    vm.dirty_ratio = 90  
    
    vm.dirty_writeback_centisecs = 5000  //50秒
    vm.dirty_expire_centisecs = 30000   //300秒
    
### 面试题：Buffered IO和普通IO谁快？

Buffered IO更快；

因为JVM中Buffered IO是8KB，8KB数组，8KB满了以后调一次内核的syscall.write(也就是做一次==系统调用==)将8KB的数组内容

而普通IO，每次write的时候，就会做一次系统调用

**Buffered IO和普通IO，write的时候，内核切换的次数不一样**


### ByteBuffer（堆外内存 VS 堆内内存） VS  MappedByteBuffer

![206](FC7FA995E82C48FBAA740D89FC1C8518)

**堆外内存 VS 堆内内存**

jvm的堆在java进程(linux系统的进程)的堆里

堆内【ByteBuffer.allocate()】：说的是在jvm的heap里的字节数组；

堆外【ByteBuffer.allocateDirect()】：说的是在jvm的heap之外，在java进程之内

堆外内存 比 堆内内存 性能稍高一些；因为堆内分配要先转化成堆外分配，才能到达内核的pagecache

**应用场景**：如果现在有一个Object对象是你自己的，是你能控制的，那么可以直接分配堆外内存；如果不能控制，则使用堆内内存（目前没有很好的案例）

---

**ByteBuffer  VS  MappedByteBuffer**


ByteBuffer（堆外内存、堆内内存）【channel.read() 、 channel.write()】访问内核的pagecache，**需要经过系统调用**

MappedByteBuffer访问pagecache【MappedByteBuffer.put()】，**不要经过系统调用**；因为maped映射

**mapped映射**（MappedByteBuffer）：通过mmap系统调用得到 MappedByteBuffer（也是字节数组），逻辑地址是映射到内核的pagecache，**是一个进程和内核共享的内存区域**，且这个内存区域是pagecache到磁盘文件的映射

只有文件通道(FileChannel)有map()方法；**文件是块设备，可以自由寻址** ；socket或serverSocket是没有map方法的，因为是**字符设备，不能自由漂移**

**性能**：==on heap < off heap  < mapped(只限文件)==

==MappedByteBuffer相比堆内分配和堆外分配，性能要高太多了==

**场景使用：**

==netty既可以on heap,又可以off heap;off heap更多一些==

==kafka log：mmap==




```java
ByteBuffer buffer = ByteBuffer.allocate(1024);//直接分配在堆上
ByteBuffer buffer = ByteBuffer.allocateDirect(1024);//分配在堆外
buffer.put("123".getBytes());
buffer.flip();   //翻转：为了读写交替
buffer.compact();
buffer.get();
buffer.clear();
System.out.println("postition: " + buffer.position());
System.out.println("limit: " +  buffer.limit());
System.out.println("capacity: " + buffer.capacity());
//mark: java.nio.DirectByteBuffer[pos=0 lim=1024 cap=1024]

//===============分割线=================

RandomAccessFile raf = new RandomAccessFile(path, "rw");//随机写，读写权限
raf.write("hello mashibing\n".getBytes());//**普通写
raf.seek(4);
raf.write("ooxx".getBytes());//内容变成 hellooxxshibing

//**堆外映射写
//只有文件通道(FileChannel)有map()方法；文件是块设备，可以自由寻址，
// socket或serverSocket是没有map方法的，因为是字符设备，不能自由漂移
FileChannel rafchannel = raf.getChannel();
MappedByteBuffer map = rafchannel.map(FileChannel.MapMode.READ_WRITE, 0, 4096);
map.put("@@@".getBytes()); //不是系统调用  但是数据会到达 内核的pagecache  
//文件内容：@@@looxxshibing
 
 ByteBuffer buffer = ByteBuffer.allocate(8192);
 int read = rafchannel.read(buffer);   //相当于buffer.put()，往buffer中put内容
 buffer.flip();
 for (int i = 0; i < buffer.limit(); i++) {
    Thread.sleep(200);
    System.out.print(((char)buffer.get(i)));
}
//打印@@@looxxshibing

```


### TCP的三次握手

TCP是面向连接的，可靠的传输协议

**三次握手之后在内核级开辟资源**，这个资源代表了连接

    三次握手：
    客户端发送syn到服务端；
    服务端发送syn+ack到客户端
    客户端再发送ack到服务器

**连接建立以后，会开辟资源（socket四元组+内存缓存区buffer[send_buf  rece_buf]）**

**客户端和服务端建立连接以后，会开辟资源，资源里面包含了一个条目（四元组：AIP_CPORT + XIP_XPORT）;这个条目在客户端和服务端的内核里都保存了这个对应关系（AIP_CPORT + XIP_XPORT）以及内存缓存区send_buf、rece_BUf(即会开辟buffer)，**

状态为SYN_RECV，说明三次握手（客户端发了syn,服务端没有发送syn+ack）只进行了一次握手

### socket(四元组+系统调用过程)

socket是一个四元组（客户端ip地址、客户端的端口号、服务端的ip地址、服务端的端口号）

==四元组让Socket变得唯一==

端口号最多有65535

socket也是内核级的；三次握手之后，即使不调用accept，也会完成socket连接的建立，以及数据的接受过程

**当accept以后，会拿到内核socket抽象的代表FD**,**在java中FD被包装为java.net.Socket**

计算机中有application应用程序，Kernel内核，无论哪种IO模型，只要想通过网络IO进行通信，application和kernel之间有一个固定的环节不被破坏的(只能修正)就是一套基本的系统调用（Socket）

    这套系统调用是：
    Socket返回文件描述符，例如FD3
    bind绑定，例如绑定到8090端口：bind(fd3,8090)
    listen监听FD3
    
    可以通过netstat -natp 进行查看结果，结果：0.0.0.0:8090  0.0.0.0:* LISTEN

![301](FDDD8BE0DBE347748A18BA8EA6413681)

服务端、客户端连接过程以及资源分配

![302](2C7F5A77D0E54A638085E25820443A9C)

### 面试题：服务器进程可不可以有XPORT(80)、YPORT（9090）？
有一个客户端和服务端；客户端的ip地址：AIP，随机端口号CPORT：123；服务端的IP地址：XIP，端口号：XPORT；请问服务器进程可不可以有XPORT(80)、YPORT（9090）？

可以

客户端的CPORT 连接 服务端的XPORT以后，会得到一个四元组（AIP_CPORT + XIP_XPORT）

### 面试题2：服务器是否需要为client的连接分配一个随机端口号

不需要

**客户端和服务端建立连接以后，会开辟资源，资源里面包含了一个条目（AIP_CPORT + XIP_XPORT）;这个条目在客户端和服务端的内核里都保存了这个对应关系（AIP_CPORT + XIP_XPORT）以及内存缓存区send_buf、rece_BUf(即会开辟buffer)，**

因为这个条目（AIP_CPORT + XIP_XPORT）是唯一标识，就没有必要再拼一个随机端口号了

### Socket-BLOCK_LOG

如果Back_log=2,可以建立的socket是3个，2个是备胎

back_log要合理配置，如果back_log的socket长时间不处理，会连接超时

## 三次握手-窗口机制 -> 解决拥塞（腾讯面试必问）

第一次：有一个seq的序列号，win 窗口大小=14600(客户端申请窗口大小)；mss=1460(数据内容大小)

第二次：回传一个seq的序列号，ack=第一次seq+1；win=1448（服务端答复）

第三次：ack=1; win=115(最终协商的窗口大小)

数据包应该多大？  利用ifconfig  查看MTU，MTU=1500字节

==数据内容大小=MTU-包头的ip(20字节)-端口号(20字节)=1500-20-20=1460==

### 腾讯面试必问：TCP中什么叫拥塞？

**拥塞：如果窗口被填满了，回的数据包里  确认的ack告诉没有余量了；**

这是客户端就可以先阻塞自己，不发了；

等服务器内核把数据处理一些以后，服务器再补一个包，通知客户端可以发了

==拥塞控制：既能提高性能，又能防止塞爆了==

==没有拥塞控制，客户端一味的发送数据，一些数据是会丢掉的==

![304](BE786857217740A7B9D9470D8E13E6EA)

### Socket-NoDelay（false:优化）

NoDelay=false  默认为优化的状态

OOBInline=false NoDelay=false两个需要组合使用才能有效果

client.setOOBInline(false);//关闭首字节快速发送

NoDelay=true 不优化，着急发送消息的时候，数据量还不大的情况下，可以不采用优化；因为这个优化，会导致吞吐量下降

### Socket-CLI_KEEPALIVE()

CLI_KEEPALIVE: keepalive的心跳是否代表长连接

==TCP协议中规定，如果双方建立了连接，很久都不说话，对方还活着吗？==

传输控制层要发一些确认包，作心跳，确认对方是否还活着

周期性做这件事，能为后期资源的把控、效率、性能有一定的帮助

注意概念别混淆：

1.HTTP也有Keepalive

2.负载均衡里有Keepalived（高可用进程）

![305](67A0FFBC96C242799D704582FF3CC557)

### =============================

### BIO

    Kenel
    socket=fd3
    bind(fd3,9090)
    listen(fd3)
    
    客户端连接
    accept(fd3 -> clone fd5 /   blocking )
    clone: recv(fd5 -> blocking
    
==accept是一个系统调用；accept会触发clone的系统调用得到Thread==

==BIO的弊端：BLOCKING；两个阻塞：accept是阻塞的， 读写的线程的读写操作也是阻塞的（阻塞是Kernel造成的）==

![307](E30CD73B105E4822BF2BE883C738E17A)
    
```
Server：
    ServerSocket server = null;
    try{
        server = new ServerSocket();
        server.bind(new InetSocketAddress(9090), BACK_LOG);
        server.setReceiveBufferSize(RECEIVE_BUFFER);
        server.setReuseAddress(REUSE_ADDR);
        server.setSoTimeout(SO_TIMEOUT);
        
        while(true){
            Socket client = server.accept();  //阻塞的，没有 -1  一直卡着不动  accept(4,
            client.setKeepAlive(CLI_KEEPALIVE);
            client.setOOBInline(CLI_OOB);
            client.setReceiveBufferSize(CLI_REC_BUF);
            client.setReuseAddress(CLI_REUSE_ADDR);
            client.setSendBufferSize(CLI_SEND_BUF);
            client.setSoLinger(CLI_LINGER, CLI_LINGER_N);
            client.setSoTimeout(CLI_TIMEOUT);
            client.setTcpNoDelay(CLI_NO_DELAY);
            new Thread(()->{
                InputStream in = client.getInputStream();
                BufferedReader reader = new BufferedReader(new InputStreamReader(in));
                char[] data = new char[1024];
                while (true) {
                    int num = reader.read(data);
                    if (num > 0) {
                        System.out.println("client read some data is :" + num + " val :" + new String(data, 0, num));
                    } else if (num == 0) {
                        System.out.println("client readed nothing!");
                        continue;
                    } else {
                        System.out.println("client readed -1...");
                        System.in.read();
                        client.close();
                        break;
                    }
                }

            }).start();
        }
    }
    
===========================
Client:
    Socket client = new Socket("192.168.150.11",9090);
    client.setSendBufferSize(20);
    client.setTcpNoDelay(true);
    OutputStream out = client.getOutputStream();
    InputStream in = System.in; //从控制台输入内容
    BufferedReader reader = new BufferedReader(new InputStreamReader(in));
    while(true){
        String line = reader.readLine();
        if(line != null ){
            byte[] bb = line.getBytes();
            for (byte b : bb) {
                out.write(b);
            }
        }
    }
```

    
当有一个连接进来的时候，主线程accept()接受，得到这个连接，并克隆出一个新的线程，在这个新的线程中receive和read，读取那个连接发送过来的数据；

因为等待连接的建立是阻塞的，读取数据是阻塞的，所以在并发的情况下，由一个线程等着连接进来，然后排队挨个处理每个到达的人，并把每个人发来的数据绑定到独立的线程当中，那么即便他们阻塞也不会阻塞其他的操作

    socket系统调用是：
    Socket返回文件描述符，例如FD3
    bind绑定，例如绑定到8090端口：bind(fd3,8090)
    listen监听FD3
    
    可以通过netstat -natp 进行查看结果，结果：0.0.0.0:8090  0.0.0.0:* LISTEN
    
    应用程序走完上面这几步，才能拿到这个Listen状态
    
    有了Listen状态，调用accept，例如accept(fd3,      
    等待客户端的连接，这时进入阻塞状态
    
    客户端连接以后，得到另一个文件描述符，例如FD5
    需要注意：有两个socket，一个是服务端的Listen状态的socket，一个客户端连接以后的socket
    
    有了FD5以后，想读，需要receive()，这时又有可能进入一个阻塞状态：等待数据传输
    
    java代码中有两处会出现阻塞：一个是等待客户端连接；一个是读取数据阻塞
    
    是如何解决这两个阻塞的呢？抛出去一个线程（进行数据的读写）
    
    主线程等待客户端连接，客户端连接以后clone一个线程
    
    clone的线程：阻塞，读取一个连接
    
![307](E30CD73B105E4822BF2BE883C738E17A)

### C10K问题

单机服务器连接1万个客户端的问题

实验：windows有一个java进程，玩命在windows调资源，开辟连接，是一个客户端想建立更多的连接数

客户端，单机单进程，循环55000次，每次创建两个客户端，相同的端口号，不同的IP地址访问同一个Server，对一台服务器发起11万的连接
突破端口号65535的限制


```java
Server:
    ServerSocket server = null;
    try {
        server = new ServerSocket();
        server.bind(new InetSocketAddress(9090), BACK_LOG); //BACK_log=2
        server.setReceiveBufferSize(RECEIVE_BUFFER); //RECEIVE_BUFFER=10
        server.setReuseAddress(REUSE_ADDR); //REUSE_ADDR=false
        server.setSoTimeout(SO_TIMEOUT);SO_TIMEOUT=0
        
        while(true){
            Socket client = server.accept();  //阻塞的，没有 -1  一直卡着不动  accept(4,
            client.setKeepAlive(CLI_KEEPALIVE); //CLI_KEEPALIVE=false
            client.setOOBInline(CLI_OOB); //CLI_OOB=false
            client.setReceiveBufferSize(CLI_REC_BUF); //CLI_REC_BUF=20
            client.setReuseAddress(CLI_REUSE_ADDR); CLI_REUSE_ADDR=20
            client.setSendBufferSize(CLI_SEND_BUF); CLI_SEND_BUF=20
            client.setSoLinger(CLI_LINGER, CLI_LINGER_N); CLI_LINGER=true  CLI_LINGER_N=0
            client.setSoTimeout(CLI_TIMEOUT); CLI_TIMEOUT=0
            client.setTcpNoDelay(CLI_NO_DELAY); CLI_NO_DELAY=false
            
            new Thread(()->{
                InputStream in = client.getInputStream();
                BufferedReader reader = new BufferedReader(new InputStreamReader(in));
                char[] data = new char[1024];
                while (true) {
                    int num = reader.read(data);
                    if (num > 0) {
                        System.out.println("client read some data is :" + num + " val :" + new String(data, 0, num));
                    } else if (num == 0) {
                        System.out.println("client readed nothing!");
                        continue;
                    } else {
                        System.out.println("client readed -1...");
                        System.in.read();
                        client.close();
                        break;
                    }
                }

            }).start();
        }

    }
    
    ===========================
    Client:
        Socket client = new Socket("192.168.150.11",9090);
        client.setSendBufferSize(20);
        client.setTcpNoDelay(true);
        OutputStream out = client.getOutputStream();
        InputStream in = System.in; //从控制台输入内容
        BufferedReader reader = new BufferedReader(new InputStreamReader(in));
        while(true){
            String line = reader.readLine();
            if(line != null ){
                byte[] bb = line.getBytes();
                for (byte b : bb) {
                    out.write(b);
                }
            }
        }

```


```shell
抓包观察：tcpdump -nn -i eth0 port 9090

```


#### 问题1：192.168.150.1：10000（本机网卡）三次握手成功了；192.168.110.100：10000（虚拟网卡）进行了前两次握手，第三次没有进行；反复进行前两次握手

    route -n 服务端查看路由表
    通过这个路由表，发现这个linux跟192.168.110.100不是直连关系;
    
    第一次握手：原地址192.168.110.100   目标地址：192.168.150.11
    第二次：原地址192.168.150.2  目标地址：192.168.110.100
    本来原地址应该为192.168.150.11,但是因为经过了NAT地址转换，原地址就由192.168.150.11变成了192.168.150.2；
    而windows并不认192.168.150.2，所以就把第二次握手的数据包给丢弃了，所以这个连接就没有建立上

#### 问题1解决

linux服务器增加一个路由器条目就可以,让 192.168.110.100走window网卡的地址 （192.168.150.1）

route add -host 192.168.110.100 gw 192.168.150.1

#### 问题2：BIO 连接慢

一个客户端经过三次握手，通过kernel连上Server main（服务端的主线程） 服务端的主线程的accept以后，抛出一个Thread（线程）用于处理数据的读写

accept是一个系统调用；accept会触发clone的系统调用得到Thread

==慢的原因就是：克隆一个线程的过程消耗了时间==；并且线程多了话，线程切换也需要时间

==优化：先准备好一个线程池，来一个连接就分配一个线程，不需要accept再clone创建出一个新的线程；这就是池化思想==


### NIO

NIO： jdk new IO;

OS NONBlocking

设置为NIO：

    ServerSocketChannel.configureBlocking(false)
    SocketChannel.configureBlocking(false);

==NIO优势：客户端的连接，以及连接建立以后的数据读写，不阻塞了==

==NIO的问题：应用程序需要全量遍历，涉及系统调用，需要用户态内核态切换才能实现==

这时候如果==发生C10K问题==，即IO的连接数量已经达到1万个，单线程每循环一次，==成本是O(n)的复杂度==，即1万次recv系统调用，涉及到用户态内核态的切换，==很多调用时无意义的，是浪费的==

![406](93F0A058CF584523AF41017E0776B98C)   

```java
Server
    LinkedList<SocketChannel> clients = new LinkedList<>();
    ServerSocketChannel ss = ServerSocketChannel.open();  //服务端开启监听：接受客户端
    ss.bind(new InetSocketAddress(9090));
    ss.configureBlocking(false); //重点  OS  NONBLOCKING!!!  //只让接受客户端  不阻塞
    
    while (true) {
        SocketChannel client = ss.accept(); //不会阻塞？  -1 NULL
        //accept  调用内核了：1，没有客户端连接进来，返回值？
        //在BIO 的时候一直卡着
        //在NIO不卡着，返回-1，NULL;如果来客户端的连接，accept 返回的是这个客户端的fd5

        if (client == null) {
            System.out.println("null.....");
        } else {
            client.configureBlocking(false); 
            //重点  连接socket<连接后的数据读写使用的> 
            int port = client.socket().getPort();
            clients.add(client);
        }

        ByteBuffer buffer = ByteBuffer.allocateDirect(4096);  //可以在堆里   堆外

        //遍历已经链接进来的客户端能不能读写数据
        for (SocketChannel c : clients) {   //串行化！！！！  多线程！！
            int num = c.read(buffer);  // >0  -1  0   //不会阻塞
            if (num > 0) {
                buffer.flip();
                byte[] aaa = new byte[buffer.limit()];
                buffer.get(aaa);

                String b = new String(aaa);
                System.out.println(c.socket().getPort() + " : " + b);
                buffer.clear();
            }

        }
    }

```

```shell
javac SocketNIO.java  &&  strace -ff -o out java SocketNIO

```

#### 问题1：在连接端口到达12000的时候，连接速度变慢，最后在端口号=12044时会报错：Exception in thread "main" java.io.IOException:Too many open files，超出文件描述符的数量 -> ==迭代client时调用read(),涉及用户态内核态的切换==

**原因分析**：因为在while(true)中，要对LinkedList<SocketChannel> clients进行遍历，遍历的时候，要调用read();==read方法要进行系统调用，涉及到内核态和用户态的切换==

#### 解决Too many open file  -》 ulimit -SHn 50000

ulimit -a 可以看到open files= 1024 （open files：一个进程可以打开多少个文件描述符）

**注意：如果是非root用户，一个进程最多可以打开1024个FD；root用户不是这个限制**

**解决**：ulimit -SHn 500000 设置open files可以接收50万

ulimit -n 查看设置的参数，open files=50万

### 问题：使用ulimit设置，与修改内核参数/proc/sys/fs/file-max有什么区别？

我本机的虚拟机的内存是4G，通过命令cat /proc/sys/fs/file-max 得到的结果是385915
    
linux内核根据物理内存，在内核级别估算出的；1G内存一般对应10万个文件描述符
    
==/proc/sys/fs/file-max是OS kenerl能够开辟的文件描述符的数量==
    
==ulimit是控制不同用户它的进程能够申请的文件描述符的上限==

补充：在 /etc/security/limits.conf有一个属性nproc  (max number of processes 打开的最大进程数)，如果java线程创建不出来了，有可能是这个设置影响的

### 多路复用

多条路(IO)通过一个系统调用，获得其中的IO状态，然后由程序自己对有状态的IO进行读写

只关注IO，不关注从IO读写完之后的事情

==注意：只要程序自己读写，那么你的**IO模型**就是同步的==

**区别于：读取多个IO的数据后，在多个线程里处理，那是处理的同步异步问题**

![502](52E951F53CCE453F9A1A4AF9749F88F2)

### 多路复用的实现：select、poll、epoll

SELECT（POSIX标准）<所有操作系统都会提供的一种多路复用器>

POLL

EPOLL

==上面的三种实现==，如果面试问到，记住是==同步模型下非阻塞的一种使用==

==SELECT是相对容易实现的，不同的操作系统内核实现方式约束力很少，是解耦性很强的一个系统调用，是所有操作系统都会提供的，都遵循POSIX组织的规范==

==EPOLL需要内核具备一定的实现；linux是EPOLL,Unix是kqueue，它们都是基于IO事件的一种通知行为，都是同步的==

![503](49031FB0A7794EAA9325E014129A67DA)




### 中断：硬、软、IO中断

==软中断-中断向量表==：cpu的寄存器 255个整数 1到255；  ==int 80==：callback函数

==硬中断-时钟中断==：CPU中有==晶振==，过来的电压，出去的电压‘哒哒哒’，1秒中会规则的震荡，每哒一次产生时钟中断，会去找中断向量表相应的回调函数，可以做一些计数器或做一些系统调用；

时钟中断可以促使我们即使只有1核CPU，也可以在内核，程序、程序之间进行交替

时钟中断的应用场景：只有1核CPU，启动了很多应用程序，应用程序的交替执行就使用了时钟中断    切片   

IO中断：

网卡、键盘、鼠标都是输入输出设备；

鼠标会卡顿

网卡：有数据包从客户端进到网卡，这时候有几种可能性，

第一种：一个数据包低速进入，这时网卡发生了IO中断，这个中断一定会打算CPU，cpu就不去处理程序，这个程序没有进入挂起状态，未来cpu处理完中断程序继续执行；另外打断CPU，cpu把读到的数据复制到内存中；这时最早的情况

网卡中有一个==buffer==，这个buffer可以被cpu读取；另外内存中可以再开辟一个空间，叫==DMA==，内核中有网卡驱动，网卡驱动被激活会开启一个DMA

网卡触发IO中断有三个级别：

1.package：来个一个包就触发

2.buffer：攒一个buffer，放到DMA中，使用最常见的

3.轮询：数据来的太快，就会关闭中断，cpu停止靠中断来同步数据；直接通过轮训读取DMA的数据

这是由网卡的工作频率，内核动态调整；

在EPOLL之前的callback：只是完成了将网卡发来的数据，走内核网络协议栈（包含2/3/4层网络协议<链路层、网络层、传输控制层>），最终关联到FD的buffer，所以你某一个时间从APP询问内核某一个或某些FD是否可读可写，会有状态返回


![504](8BAA6B746384404AA75A6C407C16C5BE)

### 中断延伸

客户端的数据包 到达 网卡，网卡根据中断里面的callback，将发送来的数据包放到FD4的缓冲区buffer

延伸 要处理什么事情呢？拿着这个fd4去红黑树中去找，如果找到，将fd4拷贝到链表中了

### Epoll

==EPOLL执行快的原因就是：规避了遍历==

系统调用：epoll_create(2)、epoll_ctl(2)、epoll_wait（2）

这三个系统调用就是在完成两个弊端的修正，以及内核里底层实现了基于callback 以及文件描述符迁移的过程，

**epoll_create**如果成功返回一个文件描述符（epfd：epoll+fd）fd6,在内核开辟一个空间，这个空间就是fd6, 这个空间存放红黑树

**epoll_ctl**(fd6,ADD,fd4,accept) 相当于图中的epoll_ctl(fd6,ADD,fd4) -> fd4 ->accept

    第一个参数：epoll_create开辟的空间fd6
    第二个参数op：执行的操作（EPOLL_CTL_ADD、EPOLL_CTL_MOD、EPOLL_CTL_DEL）
    第三个参数fd:具体的文件描述符，可以是listen的fd，也可以是socket
    第四个参数epoll_event:关注的事件，比如读、写等等
    
**epoll_wait**：调了epoll_wait以后等待有状态事件的返回 , 也就是在等一个链表

epoll_wait调到内核了，内核中会有一个链表，链表中的东西来自于红黑树向它的状态的迁移、拷贝

![505](3FDF9B84CCD0441BBC45CE0E9A144711)

### 问题：什么时候将FD从红黑树拷贝到链表中呢  -》 中断处理延伸

###  NIO VS select VS poll 

select:有进程打开文件描述符的大小限制，1024；遵循POSIX规范，所有操作系统都提供

poll: 没有大小限制；linux实现：epoll；unix实现：kqueue

时间复杂度：O(1)+O(m)

while循环中调用select(fds) [复杂度：O(1)]后知道所有持有的连接中哪些是可读写的，然后对可读写的连接进行recv() [时间复杂度：O(m),不是O(n)]


==**<含金量很高的一句话>:其实，无论NIO、SELECT、POLL都是要遍历所有的IO，询问IO的状态**==

==只不过，NIO这个遍历的过程，都需要用户态内核态的切换==

==在SELECT、POLL(多路复用器-1阶段)的情况下：这个遍历的过程，触发了一次系统调用，即一次用户态内核态的切换，过程中把fds传递给内核，内核重新根据用户这次调用传过来的fds，遍历并修改状态==

**SELECT、POLL的弊端**：

> 1.每次都要重新传递fds（epoll_create内核开辟空间）
> 
> 2.每次内核被调用以后，针对这次调用触发一个遍历fds全量的复杂度


### EPOLL V.S  SELECT

SELECT:站在程序的角度，什么时候调用select，内核什么时候遍历fds修正状态

EPOLL:内核中，程序在红黑树中放过一些FD，那么伴随内核基于中断处理完fd的buffer，状态呀；之后，继续把有状态的fd，copy到链表中；所以，程序只要调用wait，就能及时取走有状态的FD的结果集

**epoll_wait得到的结果集，程序还需要进行处理（accept、recv），所以Epoll_wait还是同步模型**

epoll_wait拿到有状态的FDs,还需要app做read、write操作

==epoll_wait 约等于select/Poll，只不过epoll_wait不传递参数fds，也不触发内核遍历==

==epoll相比select多了epoll_create、epoll_ctl两个调用、就是为了内核开辟空间的维护，以及这个开辟空间基于系统中断延伸向链表迁移的过程==

==EPOLL和SELECT通用的是：即便你不调用我内核，我内核也会随着中断完成所有FD的状态设置==


![506](7AC068E6472F4358A7F133B36A2B9006)


### selector（多路复用器-单线程解决并发的代码必须会，必须手撕）

```
package com.bjmashibing.system.io;

import java.io.IOException;
import java.net.InetSocketAddress;
import java.nio.ByteBuffer;
import java.nio.channels.*;
import java.util.HashMap;
import java.util.Iterator;
import java.util.Set;

public class SocketMultiplexingSingleThreadv1 {

    //马老师的坦克 一 二期
    private ServerSocketChannel server = null;
    private Selector selector = null;   //linux 多路复用器（select poll    epoll kqueue） nginx  event{}
    int port = 9090;

    public void initServer() {
        try {
            server = ServerSocketChannel.open();
            server.configureBlocking(false);
            server.bind(new InetSocketAddress(port));


            //如果在epoll模型下，open--》  epoll_create -> fd3
            selector = Selector.open();  //  select  poll  *epoll  优先选择：epoll  但是可以 -D修正

            //server 约等于 listen状态的 fd4
            /*
            register
            如果：
            select，poll：jvm里开辟一个数组 fd4 放进去
            epoll：  epoll_ctl(fd3,ADD,fd4,EPOLLIN
             */
            server.register(selector, SelectionKey.OP_ACCEPT);


        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public void start() {
        initServer();
        System.out.println("服务器启动了。。。。。");
        try {
            while (true) {  //死循环

                Set<SelectionKey> keys = selector.keys();
                System.out.println(keys.size()+"   size");


                //1,调用多路复用器(select,poll  or  epoll  (epoll_wait))
                /*
                select()是啥意思：
                1，select，poll  其实  内核的select（fd4）  poll(fd4)
                2，epoll：  其实 内核的 epoll_wait()
                *, 参数可以带时间：没有时间，0  ：  阻塞，有时间设置一个超时
                selector.wakeup()  结果返回0

                懒加载：
                其实再触碰到selector.select()调用的时候触发了epoll_ctl的调用

                 */
                while (selector.select() > 0) {
                    Set<SelectionKey> selectionKeys = selector.selectedKeys();  //返回的有状态的fd集合
                    Iterator<SelectionKey> iter = selectionKeys.iterator();
                    //so，管你啥多路复用器，你呀只能给我状态，我还得一个一个的去处理他们的R/W。同步好辛苦！！！！！！！！
                    //  NIO  自己对着每一个fd调用系统调用，浪费资源，那么你看，这里是不是调用了一次select方法，知道具体的那些可以R/W了？
                    //幕兰，是不是很省力？
                    //我前边可以强调过，socket：  listen   通信 R/W
                    while (iter.hasNext()) {
                        SelectionKey key = iter.next();
                        iter.remove(); //set  不移除会重复循环处理
                        if (key.isAcceptable()) {
                            //看代码的时候，这里是重点，如果要去接受一个新的连接
                            //语义上，accept接受连接且返回新连接的FD对吧？
                            //那新的FD怎么办？
                            //select，poll，因为他们内核没有空间，那么在jvm中保存和前边的fd4那个listen的一起
                            //epoll： 我们希望通过epoll_ctl把新的客户端fd注册到内核空间
                            acceptHandler(key);
                        } else if (key.isReadable()) {
                            readHandler(key);  //连read 还有 write都处理了
                            //在当前线程，这个方法可能会阻塞  ，如果阻塞了十年，其他的IO早就没电了。。。
                            //所以，为什么提出了 IO THREADS
                            //redis  是不是用了epoll，redis是不是有个io threads的概念 ，redis是不是单线程的
                            //tomcat 8,9  异步的处理方式  IO  和   处理上  解耦
                        }
                    }
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public void acceptHandler(SelectionKey key) {
        try {
            ServerSocketChannel ssc = (ServerSocketChannel) key.channel();
            SocketChannel client = ssc.accept(); //来啦，目的是调用accept接受客户端  fd7
            client.configureBlocking(false);

            ByteBuffer buffer = ByteBuffer.allocate(8192);  //前边讲过了

            // 0.0  我类个去
            //你看，调用了register
            /*
            select，poll：jvm里开辟一个数组 fd7 放进去
            epoll：  epoll_ctl(fd3,ADD,fd7,EPOLLIN
             */
            client.register(selector, SelectionKey.OP_READ, buffer);
            System.out.println("-------------------------------------------");
            System.out.println("新客户端：" + client.getRemoteAddress());
            System.out.println("-------------------------------------------");

        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public void readHandler(SelectionKey key) {
        SocketChannel client = (SocketChannel) key.channel();
        ByteBuffer buffer = (ByteBuffer) key.attachment();
        buffer.clear();
        int read = 0;
        try {
            while (true) {
                read = client.read(buffer);
                if (read > 0) {
                    buffer.flip();
                    while (buffer.hasRemaining()) {
                        client.write(buffer);
                    }
                    buffer.clear();
                } else if (read == 0) {
                    break;
                } else {
                    client.close();
                    break;
                }
            }
        } catch (IOException e) {
            e.printStackTrace();

        }
    }

    public static void main(String[] args) {
        SocketMultiplexingSingleThreadv1 service = new SocketMultiplexingSingleThreadv1();
        service.start();
    }
}

```


```
package com.bjmashibing.system.io.mylearn;


/**
 * @ClassName SocketMultiplexingSingleThread
 * @Description: TODO
 * @Author 李淼
 * @Date 2020/10/4
 * @Version V1.0
 **/
public class SocketMultiplexingSingleThread {
    private ServerSocketChannel server  =null;
    private Selector selector = null;
    int port =9090;

    public void initServer(){
        try {
            server = ServerSocketChannel.open();
            server.configureBlocking(false);
            server.bind(new InetSocketAddress(port));
            selector = Selector.open();
            server.register(selector, SelectionKey.OP_ACCEPT);

        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public void start(){
        initServer();
        System.out.println("服务器启动了");
        try {
            while (true){
                if(selector.select()>0){
                    Set<SelectionKey> selectionKeys = selector.selectedKeys();
                    Iterator<SelectionKey> iterator = selectionKeys.iterator();
                    while (iterator.hasNext()){
                        SelectionKey key = iterator.next();
                        iterator.remove();
                        if(key.isAcceptable()){
                            acceptHandler(key);
                        }else if (key.isReadable()){
                            readHandler(key);
                        }else if(key.isWritable()){
                            writeHandle(key);
                        }
                    }
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public void acceptHandler(SelectionKey key) {
        ServerSocketChannel ssc  = (ServerSocketChannel) key.channel();
        try {
            SocketChannel client = ssc.accept();
            client.configureBlocking(false);
            ByteBuffer buffer = ByteBuffer.allocate(8192);
            client.register(selector,SelectionKey.OP_READ,buffer);
            System.out.println("=======================");
            System.out.println("新客户端："+client.getRemoteAddress());
            System.out.println("------------------------");
        } catch (IOException e) {
            e.printStackTrace();
        }
    }

    public void readHandler(SelectionKey key) {
        System.out.println("readHandler.............");
        SocketChannel client = (SocketChannel) key.channel();
        ByteBuffer buffer = (ByteBuffer) key.attachment();
        buffer.clear();
        int read=0;

        try {
            while(true){
                read = client.read(buffer);
                if(read>0){
                    /*buffer.flip();
                    while (buffer.hasRemaining()) {
                        System.out.print("readContext:");
                        for (int i = 0; i < buffer.limit(); i++) {
                            System.out.print(((char)buffer.get(i)));
                        }
                        client.write(buffer);
                    }
                    buffer.clear();*/

                   /* System.out.print("readContext:");
                    for (int i = 0; i < buffer.limit(); i++) {
                        System.out.print(((char)buffer.get(i)));
                    }*/
                    client.register(selector,SelectionKey.OP_WRITE,buffer);
                }else if(read ==0){
                    break;
                }else{
                    client.close();
                    break;
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }

    }

    private void writeHandle(SelectionKey key) {
        System.out.println("writehandler............");
        SocketChannel client = (SocketChannel) key.channel();
        ByteBuffer buffer = (ByteBuffer) key.attachment();
        buffer.flip();
        while (buffer.hasRemaining()){
            try {

               /* System.out.print("writecontext:");
                for (int i = 0; i < buffer.limit(); i++) {
                    System.out.print(((char)buffer.get(i)));
                }*/
                client.write(buffer);
            } catch (IOException e) {
                e.printStackTrace();
            }
        }

        try {
            Thread.sleep(2000);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }

        buffer.clear();
        key.cancel();

        try {
            client.close();
        } catch (IOException e) {
            e.printStackTrace();
        }

    }

    public static void main(String[] args) {
        SocketMultiplexingSingleThread thread  = new SocketMultiplexingSingleThread();
        thread.start();
    }



}

```


```
package com.bjmashibing.system.io.mylearn;


public class SocketClient {

    public static void main(String[] args) {

        try {
            Socket client = new Socket("192.168.1.4",9090);

            client.setSendBufferSize(20);
            client.setTcpNoDelay(true);
            OutputStream out = client.getOutputStream();

            InputStream in = System.in;
            BufferedReader reader = new BufferedReader(new InputStreamReader(in));

            while(true){
                String line = reader.readLine();
                System.out.println("context:"+line);
                if(line != null ){
                    byte[] bb = line.getBytes();
                    for (byte b : bb) {
                        out.write(b);
                    }
                }
            }
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

```


```
package com.bjmashibing.system.io.mylearn;


/**
 * @ClassName C10KClient
 * @Description: TODO
 * @Author 李淼
 * @Date 2020/10/4
 * @Version V1.0
 **/
public class C10KClient {
    public static void main(String[] args) {
        LinkedList<SocketChannel> clients = new LinkedList<>();
        InetSocketAddress serverAddr = new InetSocketAddress("192.168.1.4",9090);

        for(int i=10000;i<65000;i++){
            try {
                SocketChannel client1 = SocketChannel.open();
                SocketChannel client2 = SocketChannel.open();

                client1.bind(new InetSocketAddress("192.168.1.4",i));
                client1.connect(serverAddr);
                clients.add(client1);

                /*client2.bind(new InetSocketAddress("192.168.21.1",i));
                client2.connect(serverAddr);
                clients.add(client2);*/
            } catch (IOException e) {
                e.printStackTrace();
            }
        }

        System.out.println("clients-size:"+clients.size());

        try {
            System.in.read();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}

```
